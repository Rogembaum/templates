# set count partition - by default 1 part / 1 topic
bin/kafka-topics.sh -- create \
    --partitions 12 \
    --topic tst

# replication partitions
bin/kafka-topics.sh -- create \
    --replication-factor 3 \
    --topic tst


# Переизбрание лидера без потерь
# ------------------------------
# 0 - подтверждение не требуется
# 1 - требуется 1 подтверждение
# all(-1) - запись во все синхронные партиции - ! тут важно что понимается под партициями - см. пункт (kafka) min.insync.replicas.per.topic
(producer) request.required.acks = 0

# что понимается под синхронными партициями
# 1 - одна реплика(лидер)
# 2 - две реплики (лидер + один из воркеров)
(kafka) min.insync.replicas.per.topic = topic:1
# есть соблазн, поставить на подтверждение все партиции на все воркеры - ! но если 1 нода выпадает - тогда запись не будет происходить
# то есть нельзя писать в реплику, если кол-во живых реплик меньше  insync.replicas
    # решение - держать кол-во реплик чуть бОльшим
     - insync.replicas = 2
     - replication factor = 3

# offset - Отступы
#! при выборе consumer - важно пониманить как он работает с отступами

# Автоматическое сохранение отступов
 - consumer считал данные И НЕ записал в базу --- кафка проверила консумер(он жив)  И пересохранила оффсет 
    <> получили потерю
 
 - consumer считал данные И записал в базу И упал --- кафка его проверила И не сохранила оффсет 
    <> получили дублирование данных

# Ручное сохранение (at least once)
 - consumer сам сообщает кафке о том, что он И скачал И сохранил данные --- кафка может переносить офсет
    <> вероятна ситуация когда консумер НЕ успел сообщить кафке, о том что он скачал И записал в базу

exactly once
    <> без потерь И без дублей

# Сохранение ВНЕ kafka
- сохраняем И данные И отступы ВНЕ кафка - !!! должно быть АТОМАРНО
    # сохранение в HDFS (exactly once)
    разбиваем на 2 части
        <> если какая то часть не отрабатывается - но ее может доделать другой консумер
        <> если что-то идет не так, удаляем все сообщения из косумера (то есть данные, которые были получены с последним запросом)
    1 hdfs dfs -mv /tmp/file1 /logs/file
    2 hdfs dfs -mv /tmp/file2 /logs/file
    3 hdfs dfs -mv /tmp/offsets /runtime/offsets


# ПРОБЛЕМЫ

# ЧТЕНИЕ
 - высокая нагрузка на чтение всего кластера
   - высокая нагрузка на чтение в разрезе нод
    <> повышенная нагрузка на чтение:
        - неравномерное распределены партиции
        - неравномерное распределены лидеры
    Решение - через конфиг решить какие ноды могут быть лидерами, и на каких нодах держать какие партиции

# НЕРАВНОМЕРНОЕ РАСПРЕДЕЛЕНИЕ НАГРУЗКИ
 - увеличение кол-ва реплик, возможно, поможет кафка не ошибаться с выбором лидер-воркера или при распределении партиций

# МОНИТОРИНГ
- сеть
- диски
- распределение нагрузки по нодам
- распределение лидеров толстых топиков
- переизбрание лидеров
- рассинхронизация партиций
    - число несинхронных партиций (UnderReplicatedPartitions)
    - максимальнй лаг репликации (ReplicaFetcherManager.MaxLag)
        <> периодически лаг будет появляться и это нормально, но если все время > 0, это уже плохо
- время ответа на запросы consumer'a